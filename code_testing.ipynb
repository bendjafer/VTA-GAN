{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230e9b78",
   "metadata": {},
   "source": [
    "# OCR-GAN Video Model Testing\n",
    "\n",
    "This notebook contains essential tests for the OCR-GAN video model with tqdm progress bars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea71737",
   "metadata": {},
   "source": [
    "## üìÑ **Research Paper Foundation: Omni-frequency Channel-selection Representations**\n",
    "\n",
    "### üî¨ **Theoretical Background**\n",
    "\n",
    "The OCR-GAN Video model is based on the research paper **\"Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection\"** which introduces a novel approach to video anomaly detection through **multi-frequency feature learning** and **adaptive channel selection**.\n",
    "\n",
    "#### **üéØ Core Research Contributions:**\n",
    "\n",
    "1. **üåä Omni-frequency Feature Learning**\n",
    "   - **Laplacian Stream**: Captures **high-frequency details** (edges, textures)\n",
    "   - **Residual Stream**: Captures **low-frequency structures** (global patterns, shapes)\n",
    "   - **Multi-scale Processing**: Combines both streams for comprehensive representation\n",
    "\n",
    "2. **üîÄ Channel Selection Mechanism (CS)**\n",
    "   - **Adaptive weighting** between Laplacian and Residual streams\n",
    "   - **Attention-based selection** using Global Average Pooling (GAP)\n",
    "   - **Context-aware** feature mixing based on input characteristics\n",
    "\n",
    "3. **üé¨ Temporal Video Processing**\n",
    "   - **16-frame snippets** for temporal context modeling\n",
    "   - **Frame-wise processing** with temporal consistency\n",
    "   - **Video-level anomaly scoring** through reconstruction error\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ **Mathematical Framework**\n",
    "\n",
    "#### **1. Omni-frequency Decomposition:**\n",
    "\n",
    "For input video frame $I \\in \\mathbb{R}^{C \\times H \\times W}$:\n",
    "\n",
    "```,\n",
    "Fused Features:        F = x‚ÇÅ + x‚ÇÇ\n",
    "Global Context:        G = GAP(F) ‚àà ‚Ñù·∂ú\n",
    "Attention Weights:     Œ± = Softmax(FC(G)) ‚àà ‚Ñù¬≤À£·∂ú\n",
    "Output:               y‚ÇÅ = Œ±‚ÇÅ ‚äô x‚ÇÅ,  y‚ÇÇ = Œ±‚ÇÇ ‚äô x‚ÇÇ\n",
    "```\n",
    "\n",
    "#### **3. Video-level Loss Function:**\n",
    "\n",
    "```,\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "2\n",
    "3\n",
    ",\n",
    ",\n",
    ",\n",
    "2\n",
    ",\n",
    ",\n",
    ",\n",
    "\n",
    ",\n",
    "1\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "2\n",
    ",\n",
    ",\n",
    ",\n",
    "3\n",
    ",\n",
    "2\n",
    ",\n",
    ",\n",
    "4\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "1\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "64\n",
    ",\n",
    ",\n",
    "16\n",
    "3\n",
    "64\n",
    "64\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "üöÄ Training Progress\")\n",
    "for epoch in epoch_bar:\n",
    "    train_bar = tqdm(enumerate(self.data.train, 0), \n",
    "                    total=len(self.data.train),\n",
    "                    desc=f\"üìà Training Epoch {epoch+1}\")\n",
    "    \n",
    "    for i, data in train_bar:\n",
    "        self.set_input(data)\n",
    "        self.optimize_params()\n",
    "        errors = self.get_errors()\n",
    "        \n",
    "        # Update progress bar with loss info\n",
    "        train_bar.set_postfix({\n",
    "            'G_loss': f\"{errors['err_g']:.4f}\",\n",
    "            'D_loss': f\"{errors['err_d']:.4f}\"\n",
    "        })\n",
    "```\n",
    "\n",
    "#### **6. Channel Shuffling Enhancement:**\n",
    "\n",
    "The **Channel Shuffling (CS)** in `UnetGenerator_CS` improves feature mixing:\n",
    "\n",
    "```python\n",
    "class UnetSkipConnectionBlock_CS(nn.Module):\n",
    "    def __init__(self, layer_id, outer_nc, inner_nc, ...):\n",
    "        # Channel shuffling applied at specific layers\n",
    "        if self.training and layer_id in shuffle_layers:\n",
    "            self.apply_channel_shuffle = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.apply_channel_shuffle:\n",
    "            x = self.channel_shuffle(x)  # Mix channels\n",
    "        # ... rest of U-Net block processing\n",
    "```\n",
    "\n",
    "#### **7. Video-Specific Anomaly Detection:**\n",
    "\n",
    "```python\n",
    "def test(self):\n",
    "    # Process each video snippet\n",
    "    for i, data in enumerate(self.data.valid):\n",
    "        self.set_input(data)\n",
    "        fake = self.netg(self.input_lap)  # Reconstruct video\n",
    "        \n",
    "        # Calculate reconstruction error per snippet\n",
    "        error = torch.mean((fake - self.input_lap) ** 2, dim=[1,2,3,4])\n",
    "        self.an_scores[i] = error  # Anomaly score\n",
    "    \n",
    "    # Compute metrics\n",
    "    auc = roc(self.gt_labels, self.an_scores)\n",
    "```\n",
    "\n",
    "### üîç **Key Technical Features:**\n",
    "\n",
    "- **Temporal Consistency**: 16-frame processing maintains temporal relationships\n",
    "- **Multi-Stream Processing**: Separate Laplacian and residual feature streams\n",
    "- **Adaptive Channel Mixing**: Channel shuffling improves feature representations\n",
    "- **Progressive Loss Weighting**: Multiple loss components balance reconstruction vs realism\n",
    "- **Real-Time Monitoring**: tqdm integration provides training visibility\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c1419",
   "metadata": {},
   "source": [
    "## üîÄ **Channel Shuffling (CS) Module Deep Dive**\n",
    "\n",
    "The **Channel Shuffling (CS)** module is a key innovation in this OCR-GAN Video architecture that enhances feature mixing between the dual-stream processing (Laplacian and Residual streams). Let's break down exactly how it works:\n",
    "\n",
    "### üèóÔ∏è **CS Module Architecture**\n",
    "\n",
    "```python\n",
    "class CS(nn.Module):\n",
    "    def __init__(self, features, WH, r, L=32):\n",
    "        super(CS, self).__init__()\n",
    "        d = max(int(features/r), L)  # Reduction dimension\n",
    "        self.gap = nn.AvgPool2d(int(WH))  # Global Average Pooling\n",
    "        self.fc = nn.Linear(int(features), d)  # First FC layer\n",
    "        self.fcs = nn.ModuleList([])  # Two separate FC layers\n",
    "        for i in range(2):\n",
    "            self.fcs.append(nn.Linear(d, features))\n",
    "        self.softmax = nn.Softmax(dim=1)  # Attention weights\n",
    "```\n",
    "\n",
    "### üîç **How CS Works - Step by Step**\n",
    "\n",
    "#### **1. Input Processing**\n",
    "The CS module takes **two feature maps** as input:\n",
    "- `x1`: Laplacian stream features (edge/boundary information)\n",
    "- `x2`: Residual stream features (temporal difference patterns)\n",
    "\n",
    "#### **2. Feature Fusion & Global Context**\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x1, x2 = x  # Separate the two streams\n",
    "    x = x1 + x2  # Element-wise addition for global context\n",
    "    \n",
    "    # Global Average Pooling - reduces spatial dimensions to 1x1\n",
    "    fea_s = self.gap(x).squeeze_()  # Shape: (batch_size, features)\n",
    "```\n",
    "\n",
    "#### **3. Attention Weight Generation**\n",
    "```python\n",
    "    # First reduction layer\n",
    "    fea_z = self.fc(fea_s.cpu())  # Shape: (batch_size, d)\n",
    "    \n",
    "    # Generate attention vectors for each stream\n",
    "    for i, fc in enumerate(self.fcs):\n",
    "        vector = fc(fea_z).unsqueeze_(dim=1)\n",
    "        if i == 0:\n",
    "            attention_vec = vector\n",
    "        else:\n",
    "            attention_vec = torch.cat([attention_vec, vector], dim=1)\n",
    "    \n",
    "    # Apply softmax to create normalized attention weights\n",
    "    attention_vec = self.softmax(attention_vec)  # Shape: (batch_size, 2, features)\n",
    "```\n",
    "\n",
    "#### **4. Adaptive Feature Weighting**\n",
    "```python\n",
    "    # Reshape for broadcasting\n",
    "    attention_vec = attention_vec.unsqueeze(-1).unsqueeze(-1)\n",
    "    attention_vec = attention_vec.transpose(0,1).to(device)\n",
    "    \n",
    "    # Apply attention weights to each stream\n",
    "    out_x1 = x1 * attention_vec[0]  # Weighted Laplacian features\n",
    "    out_x2 = x2 * attention_vec[1]  # Weighted Residual features\n",
    "    \n",
    "    return (out_x1, out_x2)\n",
    "```\n",
    "\n",
    "### üéØ **Key Benefits of CS Module**\n",
    "\n",
    "#### **1. üß† Adaptive Feature Selection**\n",
    "- **Learns which stream is more important** for each spatial location\n",
    "- **Dynamic weighting** based on input content\n",
    "- **Context-aware** feature mixing\n",
    "\n",
    "#### **2. üîÑ Cross-Stream Information Exchange**\n",
    "- Combines information from **both streams** (x1 + x2) to generate attention\n",
    "- Each stream gets **informed by the other** through shared attention computation\n",
    "- Maintains **stream identity** while enabling **cross-pollination**\n",
    "\n",
    "#### **3. üìè Spatial Invariance**\n",
    "- Uses **Global Average Pooling** to capture overall feature statistics\n",
    "- Attention weights are **spatially uniform** but **channel-specific**\n",
    "- Focuses on **what features matter** rather than **where they are**\n",
    "\n",
    "### üîß **Technical Parameters**\n",
    "\n",
    "| Parameter | Purpose | Typical Value |\n",
    "|-----------|---------|---------------|\n",
    "| `features` | Number of input channels | 64, 128, 256, 512 |\n",
    "| `WH` | Spatial dimension for GAP | 4, 8, 16, 32, 64 |\n",
    "| `r` | Reduction ratio | 2 (reduces complexity) |\n",
    "| `L` | Minimum reduction dimension | 32 (prevents over-reduction) |\n",
    "\n",
    "### üåä **Integration in U-Net Flow**\n",
    "\n",
    "```\n",
    "Input Streams (Laplacian, Residual)\n",
    "        ‚Üì\n",
    "   Downsampling Layers\n",
    "        ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      CS Module          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ Global Average Pool ‚îÇ ‚îÇ  Captures global context\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ           ‚Üì             ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ Attention Generation‚îÇ ‚îÇ  Learns importance weights\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ           ‚Üì             ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ Adaptive Weighting  ‚îÇ ‚îÇ  Applies learned weights\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚Üì\n",
    "Enhanced Feature Streams\n",
    "        ‚Üì\n",
    "   Upsampling Layers\n",
    "```\n",
    "\n",
    "### üí° **Why This Design Works**\n",
    "\n",
    "1. **üé≠ Complementary Streams**: Laplacian (edges) and Residual (motion) capture different aspects\n",
    "2. **ü§ù Intelligent Fusion**: CS learns when to emphasize each stream\n",
    "3. **üéØ Context Awareness**: Global pooling provides scene-level understanding\n",
    "4. **‚öñÔ∏è Balanced Learning**: Softmax ensures attention weights sum to 1\n",
    "5. **üîÑ Feedback Loop**: Each stream benefits from the other's information\n",
    "\n",
    "### üìä **Example Attention Behavior**\n",
    "\n",
    "- **High-motion scenes**: CS might emphasize residual stream (temporal changes)\n",
    "- **Detailed textures**: CS might emphasize Laplacian stream (edge information)\n",
    "- **Uniform regions**: CS balances both streams equally\n",
    "- **Complex scenes**: CS adaptively weights based on local feature statistics\n",
    "\n",
    "This **Channel Shuffling mechanism** is what makes the OCR-GAN Video model particularly effective at capturing both **spatial details** and **temporal dynamics** in video anomaly detection! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test with progress bars - 2 epochs for quick validation\n",
    "print(\"üöÄ Testing OCR-GAN Video Training with Progress Bars üöÄ\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Running 2 epochs with progress bars...\")\n",
    "%run train_video.py --model ocr_gan_video --dataset ucsd2 --dataroot data/ucsd2 --num_frames 8 --batchsize 1 --niter 2 --lr 0.0002 --gpu_ids -1 --ngpu 0 --device cpu --name ucsd2_quick_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training test - more epochs for complete validation\n",
    "print(\"üöÄ Full OCR-GAN Video Training Test üöÄ\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Running 5 epochs for complete training test...\")\n",
    "%run train_video.py --model ocr_gan_video --dataset ucsd2 --dataroot data/ucsd2 --num_frames 16 --batchsize 1 --niter 5 --lr 0.0002 --gpu_ids -1 --ngpu 0 --device cpu --name ucsd2_full_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tqdm progress bars functionality\n",
    "import time\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"‚úÖ Using notebook version of tqdm\")\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "    print(\"‚ö†Ô∏è Using standard tqdm (works in terminal)\")\n",
    "\n",
    "print(\"\\nTesting progress bar...\")\n",
    "for i in tqdm(range(10), desc=\"üß™ Testing Progress\"):\n",
    "    time.sleep(0.1)\n",
    "print(\"\\n‚úÖ Progress bar test completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
